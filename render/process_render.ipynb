{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import imageio\n",
    "from tqdm import tqdm\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"./render/renderings\"\n",
    "depth_scale = 1000.0\n",
    "cases = [\"sony\", \"helmet\", \"marci\", \"miyuki\", \"pigeon\", \"pine_cone\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rename the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/41 [00:00<?, ?it/s]/tmp/ipykernel_123486/2084426238.py:20: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
      "  img = imageio.imread(os.path.join(base_dir, case, img_name))\n",
      " 22%|██▏       | 9/41 [00:00<00:00, 60.86it/s]/tmp/ipykernel_123486/2084426238.py:20: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
      "  img = imageio.imread(os.path.join(base_dir, case, img_name))\n",
      "100%|██████████| 41/41 [00:00<00:00, 55.16it/s]\n"
     ]
    }
   ],
   "source": [
    "for case in cases:\n",
    "    if not os.path.isdir(os.path.join(base_dir, case)):\n",
    "        continue\n",
    "    os.makedirs(os.path.join(base_dir, case, \"images\"), exist_ok=True)\n",
    "    images = os.listdir(os.path.join(base_dir, case))\n",
    "    for img_name in tqdm(images):\n",
    "        if \"normal000\" in img_name or \"depth000\" in img_name or \"mask000\" in img_name:\n",
    "            basename, ext = os.path.splitext(img_name)\n",
    "            os.rename(\n",
    "                os.path.join(base_dir, case, img_name),\n",
    "                os.path.join(base_dir, case, \"images\", basename[:-4] + ext),\n",
    "            )\n",
    "            # print(f\"Renamd {img_name} to images/{basename[:-4] + ext}\")\n",
    "        elif img_name.endswith(\"png\"):\n",
    "            img = Image.open(os.path.join(base_dir, case, img_name))\n",
    "            img = img.convert(\"RGB\")\n",
    "            img.save(os.path.join(base_dir, case, \"images\", img_name.replace(\"png\", \"jpg\")))\n",
    "            os.rename(os.path.join(base_dir, case, img_name), os.path.join(base_dir, case, \"images\", img_name))\n",
    "        elif img_name.endswith(\"exr\") and not \"vis\" in img_name:\n",
    "            img = imageio.imread(os.path.join(base_dir, case, img_name))\n",
    "            img = img[..., :3]\n",
    "            imageio.imsave(os.path.join(base_dir, case, img_name.replace(\".exr\", \"_vis.exr\")), img)\n",
    "        # print(f\"Skipping {im}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]/tmp/ipykernel_123486/1704216580.py:6: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
      "  d_im = imageio.imread(os.path.join(base_dir, case, \"images\", d_name))\n",
      "/tmp/ipykernel_123486/1704216580.py:8: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
      "  m_im = imageio.imread(os.path.join(base_dir, case, \"images\", m_name))\n",
      "100%|██████████| 5/5 [00:00<00:00,  9.42it/s]\n"
     ]
    }
   ],
   "source": [
    "for case in cases:\n",
    "    if not os.path.isdir(os.path.join(base_dir, case)):\n",
    "        continue\n",
    "    depth_imgs = [im for im in os.listdir(os.path.join(base_dir, case, \"images\")) if \"depth.exr\" in im]\n",
    "    for d_name in tqdm(depth_imgs):\n",
    "        d_im = imageio.imread(os.path.join(base_dir, case, \"images\", d_name))\n",
    "        m_name = d_name.replace(\"depth.exr\", \"mask.png\")\n",
    "        m_im = imageio.imread(os.path.join(base_dir, case, \"images\", m_name))\n",
    "        m_im = (m_im[..., None] / 255.0).astype(np.float32)\n",
    "        d_im = d_im * m_im\n",
    "        \n",
    "        d_im = (d_im * depth_scale)[..., 0].astype(np.uint16)\n",
    "        \n",
    "        imageio.imwrite(os.path.join(base_dir, case, \"images\", d_name.replace(\"exr\", \"png\")), d_im, extension=\"png\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(meta, train_ratio=0.6, val_ratio=0.2):\n",
    "    meta_train, meta_val, meta_test = {}, {}, {}\n",
    "    for key, value in meta.items():\n",
    "        if key.startswith(\"frames\"):\n",
    "            frames = meta[\"frames\"]\n",
    "            for frame in frames:\n",
    "                frame[\"file_path\"] = \"images/\" + frame[\"file_path\"]\n",
    "            num_frames = len(frames)\n",
    "            num_train = int(num_frames * train_ratio)\n",
    "            num_val = int(num_frames * val_ratio)\n",
    "            idx = np.random.permutation(num_frames)\n",
    "            meta_train[\"frames\"] = [frames[i] for i in idx[:num_train]]\n",
    "            meta_val[\"frames\"] = [frames[i] for i in idx[num_train : num_train + num_val]]\n",
    "            meta_test[\"frames\"] = [frames[i] for i in idx[num_train + num_val :]]\n",
    "        else:\n",
    "            meta_train[key] = value\n",
    "            meta_val[key] = value\n",
    "            meta_test[key] = value\n",
    "    return meta_train, meta_val, meta_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for case in cases:\n",
    "    if not os.path.isdir(os.path.join(base_dir, case)):\n",
    "        continue\n",
    "    with open(os.path.join(base_dir, case, \"transforms.json\"), \"r\") as f:\n",
    "        meta = json.load(f)\n",
    "    meta_train, meta_val, meta_test = split_dataset(meta)\n",
    "    \n",
    "    \n",
    "    with open(os.path.join(base_dir, case, \"transforms_train.json\"), \"w\") as f:\n",
    "        json.dump(meta_train, f, indent=4)\n",
    "\n",
    "    with open(os.path.join(base_dir, case, \"transforms_val.json\"), \"w\") as f:\n",
    "        json.dump(meta_val, f, indent=4)\n",
    "        \n",
    "    with open(os.path.join(base_dir, case, \"transforms_test.json\"), \"w\") as f:\n",
    "        json.dump(meta_test, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "from arguments import ModelParams\n",
    "from eval_metrics_rotblender import get_mesh_eval_points\n",
    "from scene.gaussian_model import GaussianModel\n",
    "from argparse import ArgumentParser, Namespace\n",
    "\n",
    "from scene import Scene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found transforms_train.json file, assuming Blender data set!\n",
      "Reading Training Transforms\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './render/renderings/sony/./render/renderings/sony/images/01_000.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m cfg \u001b[38;5;241m=\u001b[39m Namespace(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcfg)\n\u001b[1;32m     15\u001b[0m gaussians \u001b[38;5;241m=\u001b[39m GaussianModel(\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m---> 16\u001b[0m scene \u001b[38;5;241m=\u001b[39m \u001b[43mScene\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgaussians\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m get_mesh_eval_points(scene, source_dir\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(base_dir, case))\n",
      "File \u001b[0;32m~/code/opensource/2d-gaussian-splatting/scene/__init__.py:59\u001b[0m, in \u001b[0;36mScene.__init__\u001b[0;34m(self, args, gaussians, bg_gaussians, load_iteration, shuffle, resolution_scales)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound transforms_train.json file, assuming Blender data set!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mrotation:\n\u001b[0;32m---> 59\u001b[0m     scene_info \u001b[38;5;241m=\u001b[39m \u001b[43msceneLoadTypeCallbacks\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mRotBlender\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msource_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwhite_background\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlinear\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapply_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_mask\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     63\u001b[0m     scene_info \u001b[38;5;241m=\u001b[39m sceneLoadTypeCallbacks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBlender\u001b[39m\u001b[38;5;124m\"\u001b[39m](\n\u001b[1;32m     64\u001b[0m         args\u001b[38;5;241m.\u001b[39msource_path, args\u001b[38;5;241m.\u001b[39mwhite_background, args\u001b[38;5;241m.\u001b[39meval, linear\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mlinear\n\u001b[1;32m     65\u001b[0m     )\n",
      "File \u001b[0;32m~/code/opensource/2d-gaussian-splatting/scene/dataset_readers.py:488\u001b[0m, in \u001b[0;36mreadRotNerfSyntheticInfo\u001b[0;34m(path, white_background, eval, extension, linear, apply_mask)\u001b[0m\n\u001b[1;32m    484\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreadRotNerfSyntheticInfo\u001b[39m(\n\u001b[1;32m    485\u001b[0m     path, white_background, \u001b[38;5;28meval\u001b[39m, extension\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m\"\u001b[39m, linear\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, apply_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    486\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SceneInfo:\n\u001b[1;32m    487\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReading Training Transforms\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 488\u001b[0m     train_cam_infos, train_bgcam_infos \u001b[38;5;241m=\u001b[39m \u001b[43mreadCamerasFromRotTransforms\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtransforms_train.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhite_background\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextension\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapply_mask\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    491\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReading Test Transforms\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    492\u001b[0m     test_cam_infos, test_bgcam_infos \u001b[38;5;241m=\u001b[39m readCamerasFromRotTransforms(\n\u001b[1;32m    493\u001b[0m         path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransforms_test.json\u001b[39m\u001b[38;5;124m\"\u001b[39m, white_background, extension, linear, apply_mask\n\u001b[1;32m    494\u001b[0m     )\n",
      "File \u001b[0;32m~/code/opensource/2d-gaussian-splatting/scene/dataset_readers.py:361\u001b[0m, in \u001b[0;36mreadCamerasFromRotTransforms\u001b[0;34m(path, transformsfile, white_background, extension, linear, apply_mask)\u001b[0m\n\u001b[1;32m    359\u001b[0m image_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(path, cam_name)\n\u001b[1;32m    360\u001b[0m image_name \u001b[38;5;241m=\u001b[39m Path(cam_name)\u001b[38;5;241m.\u001b[39mstem\n\u001b[0;32m--> 361\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    363\u001b[0m im_data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(image\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGBA\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m    365\u001b[0m bg \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m]) \u001b[38;5;28;01mif\u001b[39;00m white_background \u001b[38;5;28;01melse\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m~/miniconda3/envs/surfel_splatting/lib/python3.8/site-packages/PIL/Image.py:3247\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3244\u001b[0m     filename \u001b[38;5;241m=\u001b[39m fp\n\u001b[1;32m   3246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename:\n\u001b[0;32m-> 3247\u001b[0m     fp \u001b[38;5;241m=\u001b[39m \u001b[43mbuiltins\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3248\u001b[0m     exclusive_fp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   3250\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './render/renderings/sony/./render/renderings/sony/images/01_000.png'"
     ]
    }
   ],
   "source": [
    "for case in cases:\n",
    "    cfg = {\n",
    "        \"model_path\": \"\",\n",
    "        \"source_path\": os.path.join(base_dir, case),\n",
    "        \"white_background\": True,\n",
    "        \"eval\": False,\n",
    "        \"rotation\": True,\n",
    "        \"apply_mask\": False,\n",
    "        \"linear\": False,\n",
    "        \"shuffle\": False,\n",
    "        \"resolution\": 1,\n",
    "        \"data_device\": \"cuda\"\n",
    "    }\n",
    "    cfg = Namespace(**cfg)\n",
    "    gaussians = GaussianModel(3, -1, None, None)\n",
    "    scene = Scene(cfg, gaussians, shuffle=False)\n",
    "\n",
    "    get_mesh_eval_points(scene, source_dir=os.path.join(base_dir, case))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "threestudio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
